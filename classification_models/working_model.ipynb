{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import gensim\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "df = pd.read_csv('data.csv')\n",
    "df = df[pd.notnull(df['text'])]\n",
    "df = df[pd.notnull(df['tag'])]\n",
    "df = df.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jai06\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = BeautifulSoup(text, \"lxml\").text  # HTML decoding\n",
    "    text = text.lower()  # lowercase text\n",
    "    # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text)\n",
    "    # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = BAD_SYMBOLS_RE.sub('', text)\n",
    "    # delete stopwors from text\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
    "    return text\n",
    "\n",
    "\n",
    "df['text'] = df['text'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jai06\\AppData\\Local\\Temp\\ipykernel_46672\\2073029742.py:4: DeprecationWarning: Call to deprecated `init_sims` (Use fill_norms() instead. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  wv.init_sims(replace=True)\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "from gensim.models import Word2Vec\n",
    "wv = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "wv.init_sims(replace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = set(wv.index_to_key)\n",
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in s:\n",
    "            mean.append(wv.vectors[wv.key_to_index[word]])\n",
    "            all_words.add(wv.key_to_index[word])\n",
    "\n",
    "    if not mean:\n",
    "        # FIXME: remove these examples in pre-processing\n",
    "        return np.zeros(wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(\n",
    "        np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "\n",
    "def word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, text) for text in text_list])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens\n",
    "    \n",
    "train, test = train_test_split(df, test_size=0.3, random_state = 42)\n",
    "test_tokenized = test.apply(lambda r: w2v_tokenize_text(r['text']), axis=1).values\n",
    "train_tokenized = train.apply(lambda r: w2v_tokenize_text(r['text']), axis=1).values\n",
    "X_train_word_average = word_averaging_list(wv,train_tokenized)\n",
    "X_test_word_average = word_averaging_list(wv,test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(n_jobs=1, C=1e5, max_iter=1000000)\n",
    "logreg = logreg.fit(X_train_word_average, train['tag'])\n",
    "y_pred = logreg.predict(X_test_word_average)\n",
    "score = accuracy_score(y_pred, test['tag'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTop3(text):\n",
    "    processedInput = word_averaging(wv, w2v_tokenize_text(text))\n",
    "    prob = logreg.predict_proba([processedInput])\n",
    "    prob = prob[0]\n",
    "    labels = logreg.classes_\n",
    "    res = {}\n",
    "    for i, p in enumerate(prob):\n",
    "        res[p] = labels[i]\n",
    "    prob.sort()\n",
    "    ans = []\n",
    "    keys = prob[-3:]\n",
    "    for i in keys:\n",
    "        ans.append([res[i], i])\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['General & Admin Expenses', 0.05165550116567915],\n",
       " ['Revenue', 0.21393953507864527],\n",
       " ['Machinery & Equipment', 0.6769222315467206]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getTop3(\"laptop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Cash', 0.00030132179472606416], ['Revenue', 0.01020466634778689], ['Machinery & Equipment', 0.9890809778850956]]\n"
     ]
    }
   ],
   "source": [
    "print(getTop3(\"Computer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ML.joblib']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib_file = \"ML.joblib\"\n",
    "joblib.dump(getTop3, joblib_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "a502382c4841cc888f3e4915c077533c5da29759cb19f3e36ca430181638342e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
